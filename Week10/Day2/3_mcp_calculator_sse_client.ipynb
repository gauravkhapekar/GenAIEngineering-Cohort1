{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MCP Calculator Client with Mistral AI (SSE Version)\n",
        "\n",
        "This notebook demonstrates how to use MCP (Model Context Protocol) with Mistral AI's chat completion API using Server-Sent Events (SSE) connection.\n",
        "\n",
        "## What is SSE?\n",
        "Server-Sent Events (SSE) is a technology that allows a server to push data to web clients over HTTP. In the context of MCP, it provides a way to connect to MCP servers through HTTP endpoints instead of STDIO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "Before running this notebook, make sure you have:\n",
        "1. The required packages installed\n",
        "2. An MCP server running on `http://localhost:9321/sse`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "# !pip install mcp mistralai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Required Libraries\n",
        "\n",
        "The SSE client allows us to connect to MCP servers over HTTP, which is useful for:\n",
        "- Remote server connections\n",
        "- Web-based integrations\n",
        "- Scenarios where STDIO is not suitable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import json\n",
        "from mcp import ClientSession, StdioServerParameters\n",
        "from mcp.client.sse import sse_client\n",
        "from mcp.client.stdio import stdio_client\n",
        "from mistralai import Mistral\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure Mistral AI Client\n",
        "\n",
        "Set up your Mistral AI API key to enable LLM interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "# Set your Mistral API key\n",
        "if 'MISTRAL_KEY' not in os.environ:\n",
        "    raise ValueError(\"MISTRAL_KEY environment variable is not set.\")\n",
        "mistral = Mistral(api_key=os.getenv('MISTRAL_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect to MCP Server via SSE\n",
        "\n",
        "Unlike the STDIO version, here we connect to an HTTP endpoint. The server should be running and accessible at the specified URL.\n",
        "\n",
        "**Note**: Make sure your MCP server is running on port 9321 before executing this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  + Exception Group Traceback (most recent call last):\n",
            "  |   File \"/Users/raamraam/outskill/GenAIEngineering-Cohort1/mcp/mcp_env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3697, in run_code\n",
            "  |     await eval(code_obj, self.user_global_ns, self.user_ns)\n",
            "  |   File \"/var/folders/tm/h89czpmx6n9g4f82zh01nxsc0000gn/T/ipykernel_82853/3733295913.py\", line 5, in <module>\n",
            "  |     async with sse_client(server_params) as (read, write):\n",
            "  |   File \"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py\", line 204, in __aenter__\n",
            "  |     return await anext(self.gen)\n",
            "  |            ^^^^^^^^^^^^^^^^^^^^^\n",
            "  |   File \"/Users/raamraam/outskill/GenAIEngineering-Cohort1/mcp/mcp_env/lib/python3.11/site-packages/mcp/client/sse.py\", line 54, in sse_client\n",
            "  |     async with anyio.create_task_group() as tg:\n",
            "  |   File \"/Users/raamraam/outskill/GenAIEngineering-Cohort1/mcp/mcp_env/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n",
            "  |     raise BaseExceptionGroup(\n",
            "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
            "  +-+---------------- 1 ----------------\n",
            "    | Traceback (most recent call last):\n",
            "    |   File \"/Users/raamraam/outskill/GenAIEngineering-Cohort1/mcp/mcp_env/lib/python3.11/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n",
            "    |     yield\n",
            "    |   File \"/Users/raamraam/outskill/GenAIEngineering-Cohort1/mcp/mcp_env/lib/python3.11/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n",
            "    |     resp = await self._pool.handle_async_request(req)\n",
            "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    |   File \"/Users/raamraam/outskill/GenAIEngineering-Cohort1/mcp/mcp_env/lib/python3.11/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n",
            "    |     raise exc from None\n",
            "    |   File \"/Users/raamraam/outskill/GenAIEngineering-Cohort1/mcp/mcp_env/lib/python3.11/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n",
            "    |     response = await connection.handle_async_request(\n",
            "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    |   File \"/Users/raamraam/outskill/GenAIEngineering-Cohort1/mcp/mcp_env/lib/python3.11/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\n",
            "    |     raise exc\n",
            "    |   File \"/Users/raamraam/outskill/GenAIEngineering-Cohort1/mcp/mcp_env/lib/python3.11/site-packages/httpcore/_async/connection.py\", line 78, in handle_async_request\n",
            "    |     stream = await self._connect(request)\n",
            "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    |   File \"/Users/raamraam/outskill/GenAIEngineering-Cohort1/mcp/mcp_env/lib/python3.11/site-packages/httpcore/_async/connection.py\", line 124, in _connect\n",
            "    |     stream = await self._network_backend.connect_tcp(**kwargs)\n",
            "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    |   File \"/Users/raamraam/outskill/GenAIEngineering-Cohort1/mcp/mcp_env/lib/python3.11/site-packages/httpcore/_backends/auto.py\", line 31, in connect_tcp\n",
            "    |     return await self._backend.connect_tcp(\n",
            "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    |   File \"/Users/raamraam/outskill/GenAIEngineering-Cohort1/mcp/mcp_env/lib/python3.11/site-packages/httpcore/_backends/anyio.py\", line 113, in connect_tcp\n",
            "    |     with map_exceptions(exc_map):\n",
            "    |   File \"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py\", line 155, in __exit__\n",
            "    |     self.gen.throw(typ, value, traceback)\n",
            "    |   File \"/Users/raamraam/outskill/GenAIEngineering-Cohort1/mcp/mcp_env/lib/python3.11/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n",
            "    |     raise to_exc(exc) from exc\n",
            "    | httpcore.ConnectError: All connection attempts failed\n",
            "    | \n",
            "    | The above exception was the direct cause of the following exception:\n",
            "    | \n",
            "    | Traceback (most recent call last):\n",
            "    |   File \"/Users/raamraam/outskill/GenAIEngineering-Cohort1/mcp/mcp_env/lib/python3.11/site-packages/mcp/client/sse.py\", line 58, in sse_client\n",
            "    |     async with aconnect_sse(\n",
            "    |   File \"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py\", line 204, in __aenter__\n",
            "    |     return await anext(self.gen)\n",
            "    |            ^^^^^^^^^^^^^^^^^^^^^\n",
            "    |   File \"/Users/raamraam/outskill/GenAIEngineering-Cohort1/mcp/mcp_env/lib/python3.11/site-packages/httpx_sse/_api.py\", line 69, in aconnect_sse\n",
            "    |     async with client.stream(method, url, headers=headers, **kwargs) as response:\n",
            "    |   File \"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py\", line 204, in __aenter__\n",
            "    |     return await anext(self.gen)\n",
            "    |            ^^^^^^^^^^^^^^^^^^^^^\n",
            "    |   File \"/Users/raamraam/outskill/GenAIEngineering-Cohort1/mcp/mcp_env/lib/python3.11/site-packages/httpx/_client.py\", line 1583, in stream\n",
            "    |     response = await self.send(\n",
            "    |                ^^^^^^^^^^^^^^^^\n",
            "    |   File \"/Users/raamraam/outskill/GenAIEngineering-Cohort1/mcp/mcp_env/lib/python3.11/site-packages/httpx/_client.py\", line 1629, in send\n",
            "    |     response = await self._send_handling_auth(\n",
            "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    |   File \"/Users/raamraam/outskill/GenAIEngineering-Cohort1/mcp/mcp_env/lib/python3.11/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n",
            "    |     response = await self._send_handling_redirects(\n",
            "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    |   File \"/Users/raamraam/outskill/GenAIEngineering-Cohort1/mcp/mcp_env/lib/python3.11/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n",
            "    |     response = await self._send_single_request(request)\n",
            "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    |   File \"/Users/raamraam/outskill/GenAIEngineering-Cohort1/mcp/mcp_env/lib/python3.11/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n",
            "    |     response = await transport.handle_async_request(request)\n",
            "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "    |   File \"/Users/raamraam/outskill/GenAIEngineering-Cohort1/mcp/mcp_env/lib/python3.11/site-packages/httpx/_transports/default.py\", line 393, in handle_async_request\n",
            "    |     with map_httpcore_exceptions():\n",
            "    |   File \"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py\", line 155, in __exit__\n",
            "    |     self.gen.throw(typ, value, traceback)\n",
            "    |   File \"/Users/raamraam/outskill/GenAIEngineering-Cohort1/mcp/mcp_env/lib/python3.11/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n",
            "    |     raise mapped_exc(message) from exc\n",
            "    | httpx.ConnectError: All connection attempts failed\n",
            "    +------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# SSE server endpoint\n",
        "server_params = \"http://localhost:9321/sse\"\n",
        "\n",
        "# Connect using SSE client\n",
        "async with sse_client(server_params) as (read, write):\n",
        "    async with ClientSession(read, write) as session:\n",
        "        # Initialize the session\n",
        "        await session.initialize()\n",
        "        \n",
        "        # Discover available tools\n",
        "        tools_result = await session.list_tools()\n",
        "        print(f\"\\nðŸ§® Calculator ready! Found {len(tools_result.tools)} tools\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transform Tools for Mistral AI\n",
        "\n",
        "Just like in the STDIO version, we need to convert the MCP tool format to match Mistral's expectations. The transformation process remains the same regardless of the connection type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mistral_tools = []\n",
        "for tool in tools_result.tools:\n",
        "    tool_def = {\n",
        "        \"type\": \"function\",  # Required wrapper\n",
        "        \"function\": {        # Function definition\n",
        "            \"name\": tool.name,\n",
        "            \"description\": tool.description,\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"a\": {\n",
        "                        \"type\": \"number\",\n",
        "                        \"description\": \"First number\"\n",
        "                    },\n",
        "                    \"b\": {\n",
        "                        \"type\": \"number\",\n",
        "                        \"description\": \"Second number\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"a\", \"b\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    mistral_tools.append(tool_def)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display Available Tools\n",
        "\n",
        "Let's inspect the tools that were discovered and transformed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the transformed tools\n",
        "mistral_tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Request Calculation from Mistral\n",
        "\n",
        "Send a calculation request to Mistral. The AI will analyze the request and select the appropriate tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_input = \"Find 5 minus 2\"\n",
        "\n",
        "# Request Mistral to perform the calculation\n",
        "response = mistral.chat.complete(\n",
        "    model=\"mistral-small-latest\",\n",
        "    messages=[{\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a calculator assistant. Use the add or subtract functions to help the user with calculations.\"\n",
        "    }, {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_input\n",
        "    }],\n",
        "    tools=mistral_tools,\n",
        "    tool_choice=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect Mistral's Response\n",
        "\n",
        "Let's examine what Mistral returned, including the tool call details:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the response message\n",
        "response.choices[0].message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execute the Tool Call via SSE\n",
        "\n",
        "Now we'll execute the tool that Mistral selected. The main difference from STDIO is that we're connecting via HTTP/SSE instead of process pipes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "message = response.choices[0].message\n",
        "\n",
        "# Connect to SSE server to execute the tool\n",
        "async with sse_client(\"http://localhost:9321/sse\") as (read, write):\n",
        "    async with ClientSession(read, write) as session:\n",
        "        # Initialize session\n",
        "        await session.initialize()\n",
        "        \n",
        "        if message.tool_calls is not None:\n",
        "            for tool_call in message.tool_calls:\n",
        "                # Extract function name\n",
        "                func_name = tool_call.function.name\n",
        "                \n",
        "                # Parse arguments (handle both string and dict formats)\n",
        "                if isinstance(tool_call.function.arguments, str):\n",
        "                    args = json.loads(tool_call.function.arguments)\n",
        "                else:\n",
        "                    args = tool_call.function.arguments\n",
        "                \n",
        "                # Execute the MCP tool\n",
        "                result = await session.call_tool(func_name, args)\n",
        "                print(f\"Result: {result.content[0].text}\")\n",
        "        else:\n",
        "            print(\"No tool calls found in the response.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Comparison\n",
        "\n",
        "### SSE vs STDIO\n",
        "\n",
        "**SSE (This notebook):**\n",
        "- Connects via HTTP endpoint\n",
        "- Good for remote servers\n",
        "- Suitable for web integrations\n",
        "- Requires server to be running on a specific port\n",
        "\n",
        "**STDIO (Previous notebook):**\n",
        "- Connects via process pipes\n",
        "- Good for local servers\n",
        "- Better process isolation\n",
        "- Server runs as a subprocess\n",
        "\n",
        "Both approaches achieve the same result but are suited for different deployment scenarios."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mcp_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
